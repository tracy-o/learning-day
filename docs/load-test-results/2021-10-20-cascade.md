# Cascade performance test

We've tested the performance of the cascade functionality in the past (see
[2020-06-15-cascade.md](./2020-06-15-cascade.md) and
[2021-02-10-cascade.md](./2021-02-10-cascade.md)), however we now would like to
do it again to confirm that the performance hasn't degraded since then and no
issues have been accidentally introduced.

This means that we would basically like to repeat the previous tests, compare
the results and confirm that the performance is not worse than it was.

I thought that I would also do some extra tests and look out for any errors in
the logs and check the new metrics that have been added since the previous
tests. I don't intend to look for anything specific, but just confirm that
there are no apparent problems.

## Repeating previous tests

During the previous testing we did the following:

* Made requests to a route that was handled by two origins.
* During the first test OriginSimulator was used to simulate requests to both
  origins. It was configured to return a 404 with configured latency (100ms,
  500ms, 1000ms).
* During the second test a test lambda function that always returned a 404
  response with 500ms latency was used as the first origin and OriginSimulator
  as the second origin.  OriginSimulator was configured to return a 200 with
  some latency (100ms, 500ms, 1000ms).
* All tests lasted 60s and load was generated with a rate of 400rps.

This time I repeated both tests and the results are below.

### OriginSimulator for both origins

#### 100ms latency

```
Requests      [total, rate, throughput]         24000, 400.02, 0.00
Duration      [total, attack, wait]             1m0s, 59.997s, 204.396ms
Latencies     [min, mean, 50, 90, 95, 99, max]  204.076ms, 204.699ms, 204.699ms, 205.028ms, 205.237ms, 206.239ms, 227.533ms
Bytes In      [total, mean]                     1080000, 45.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           0.00%
Status Codes  [code:count]                      404:24000
Error Set:
404 Not Found
```

#### 500ms latency

```
Requests      [total, rate, throughput]         24000, 400.02, 0.00
Duration      [total, attack, wait]             1m1s, 59.997s, 1.004s
Latencies     [min, mean, 50, 90, 95, 99, max]  1.004s, 1.005s, 1.005s, 1.005s, 1.005s, 1.015s, 1.211s
Bytes In      [total, mean]                     1080000, 45.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           0.00%
Status Codes  [code:count]                      404:24000
Error Set:
404 Not Found
```

#### 1000ms latency

```
Requests      [total, rate, throughput]         24000, 400.02, 0.00
Duration      [total, attack, wait]             1m2s, 59.997s, 2.006s
Latencies     [min, mean, 50, 90, 95, 99, max]  2.005s, 2.008s, 2.007s, 2.008s, 2.011s, 2.018s, 3.007s
Bytes In      [total, mean]                     1080000, 45.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           0.00%
Status Codes  [code:count]                      404:24000
Error Set:
404 Not Found
```

#### Summary

These results are basically better than what we saw before: the latencies are
lower, especially with the 1000ms latency configuration.


### Test lambda + OriginSimulator

#### 100ms latency

```
Requests      [total, rate, throughput]         24000, 400.02, 395.95
Duration      [total, attack, wait]             1m1s, 59.997s, 615.839ms
Latencies     [min, mean, 50, 90, 95, 99, max]  611.067ms, 621.104ms, 616.006ms, 652.056ms, 657.821ms, 666.459ms, 710.541ms
Bytes In      [total, mean]                     48000, 2.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           100.00%
Status Codes  [code:count]                      200:24000
Error Set:
```

#### 500ms latency

```
Requests      [total, rate, throughput]         24000, 400.02, 393.35
Duration      [total, attack, wait]             1m1s, 59.998s, 1.017s
Latencies     [min, mean, 50, 90, 95, 99, max]  1.012s, 1.021s, 1.016s, 1.052s, 1.058s, 1.068s, 1.395s
Bytes In      [total, mean]                     48000, 2.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           100.00%
Status Codes  [code:count]                      200:24000
Error Set:
```

#### 1000ms latency

```
Requests      [total, rate, throughput]         24000, 400.02, 389.93
Duration      [total, attack, wait]             1m2s, 59.997s, 1.551s
Latencies     [min, mean, 50, 90, 95, 99, max]  1.512s, 1.521s, 1.516s, 1.552s, 1.558s, 1.567s, 1.602s
Bytes In      [total, mean]                     48000, 2.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           100.00%
Status Codes  [code:count]                      200:24000
Error Set:
```

#### Summary

The results are similar to what we saw before: I would say there's no
significant difference and no performance degradation.

## Extra tests

### Compare cascade and non-cascade routes

I thought it would be interesting to compare performance of a route that is
handled by the cascade, where the first origin in the cascade returns a
response and a route that is not handled by the cascade (and therefore only a
single request to origin is made). The expectation is that the latencies should
be very similar in these two cases.

The setup for these tests was OriginSimulator returning a 200 response with a
100ms latency. The tests were performed for 5 minutes with a rate of 1000rps.

#### Non-cascade route

```
Requests      [total, rate, throughput]         300000, 1000.00, 999.66
Duration      [total, attack, wait]             5m0s, 5m0s, 103.777ms
Latencies     [min, mean, 50, 90, 95, 99, max]  103.051ms, 103.903ms, 103.676ms, 104.051ms, 104.506ms, 105.121ms, 242.077ms
Bytes In      [total, mean]                     600000, 2.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           100.00%
Status Codes  [code:count]                      200:300000
Error Set:
```

#### Cascade route

```
Requests      [total, rate, throughput]         300000, 1000.00, 999.66
Duration      [total, attack, wait]             5m0s, 5m0s, 104.196ms
Latencies     [min, mean, 50, 90, 95, 99, max]  102.951ms, 104.5ms, 104.302ms, 104.611ms, 104.727ms, 105.478ms, 281.859ms
Bytes In      [total, mean]                     600000, 2.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           100.00%
Status Codes  [code:count]                      200:300000
Error Set:
```

#### Summary

The latencies during these tests are similar, which is what we expect.

### Test a cascade route with higher RPS and longer tests

I thought it would be interesting to run longer tests with higher RPS and see
if there are any unexpected errors in the logs or if anything abnormal can be
seen in the metrics. For these tests I used both setups above: OriginSimulator
for both origins in the cascade (i.e. the same as the first test above),
configured to return 404 responses with different latencies: 100ms, 500ms and
1000ms; and using a test lambda as the first origin and OriginSimulator
returning a 200 response with different latencies (100ms, 500ms and 1000ms)
(i.e. the same as the second test above). I ran these tests for 5 minutes with
rates of 800rps and 1000rps.

#### OriginSimulator for both origins

##### 100ms latency, 800rps

```
Requests      [total, rate, throughput]         240000, 800.00, 0.00
Duration      [total, attack, wait]             5m0s, 5m0s, 204.5ms
Latencies     [min, mean, 50, 90, 95, 99, max]  203.803ms, 205.107ms, 204.936ms, 205.414ms, 205.585ms, 206.439ms, 384.447ms
Bytes In      [total, mean]                     10800000, 45.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           0.00%
Status Codes  [code:count]                      404:240000
Error Set:
404 Not Found
```

##### 500ms latency, 800rps

```
Requests      [total, rate, throughput]         240000, 800.00, 0.00
Duration      [total, attack, wait]             5m20s, 5m0s, 19.769s
Latencies     [min, mean, 50, 90, 95, 99, max]  6.885ms, 17.777s, 17.114s, 30s, 30s, 30s, 30.162s
Bytes In      [total, mean]                     3850293, 16.04
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           0.00%
Status Codes  [code:count]                      0:161206  404:35285  500:43509
Error Set:
404 Not Found
500 Internal Server Error
```

##### 1000ms latency, 800rps

```
Requests      [total, rate, throughput]         240000, 800.00, 0.00
Duration      [total, attack, wait]             5m20s, 5m0s, 20.079s
Latencies     [min, mean, 50, 90, 95, 99, max]  415.934ms, 10.027s, 4.934s, 30s, 30s, 30s, 30.014s
Bytes In      [total, mean]                     8066223, 33.61
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           0.00%
Status Codes  [code:count]                      0:66135  404:139251  500:34614
Error Set:
404 Not Found
500 Internal Server Error
```

##### 100ms latency, 1000rps

```
Requests      [total, rate, throughput]         300000, 1000.00, 0.00
Duration      [total, attack, wait]             5m0s, 5m0s, 205.129ms
Latencies     [min, mean, 50, 90, 95, 99, max]  203.818ms, 205.849ms, 205.262ms, 205.584ms, 206.214ms, 206.956ms, 512.112ms
Bytes In      [total, mean]                     13500000, 45.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           0.00%
Status Codes  [code:count]                      404:300000
Error Set:
404 Not Found
```

##### 500ms latency, 1000rps

```
Requests      [total, rate, throughput]         300000, 1000.00, 0.00
Duration      [total, attack, wait]             5m29s, 5m0s, 29.142s
Latencies     [min, mean, 50, 90, 95, 99, max]  7.003ms, 19.511s, 21.21s, 30s, 30s, 30s, 30.586s
Bytes In      [total, mean]                     1960729, 6.54
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           0.00%
Status Codes  [code:count]                      0:260613  404:12485  500:26902
Error Set:
404 Not Found
500 Internal Server Error
```

##### 1000ms latency, 1000rps

```
Requests      [total, rate, throughput]         300000, 1000.00, 0.00
Duration      [total, attack, wait]             5m24s, 5m0s, 23.843s
Latencies     [min, mean, 50, 90, 95, 99, max]  6.572ms, 19.8s, 21.855s, 30s, 30s, 30s, 30.485s
Bytes In      [total, mean]                     2310198, 7.70
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           0.00%
Status Codes  [code:count]                      0:253027  404:18914  500:28059
Error Set:
404 Not Found
500 Internal Server Error
```

##### Summary

The 500s reported during these tests are `pool_full` errors. This is expected
because the same pool is used for requests to both origins (because they are
both OriginSimulator). The setup effectively means that Belfrage processes
twice as many requests than what the test rate is.

#### Test lambda + OriginSimulator

##### 100ms latency, 800rps

```
Requests      [total, rate, throughput]         240000, 800.00, 60.29
Duration      [total, attack, wait]             5m30s, 5m0s, 29.884s
Latencies     [min, mean, 50, 90, 95, 99, max]  7.429ms, 21.285s, 25.082s, 30s, 30s, 30s, 30.645s
Bytes In      [total, mean]                     850562, 3.54
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           8.29%
Status Codes  [code:count]                      0:204510  200:19889  500:15601
```

##### 500ms latency, 800rps

```
Requests      [total, rate, throughput]         240000, 800.00, 84.97
Duration      [total, attack, wait]             5m29s, 5m0s, 29.28s
Latencies     [min, mean, 50, 90, 95, 99, max]  7.551ms, 21.795s, 26.932s, 30s, 30s, 30s, 30.215s
Bytes In      [total, mean]                     1387104, 5.78
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           11.66%
Status Codes  [code:count]                      0:186423  200:27978  500:25599
```

##### 1000ms latency, 800rps

```
Requests      [total, rate, throughput]         240000, 800.00, 44.51
Duration      [total, attack, wait]             5m29s, 5m0s, 28.97s
Latencies     [min, mean, 50, 90, 95, 99, max]  1.626s, 21.868s, 26.778s, 30s, 30s, 30s, 30.02s
Bytes In      [total, mean]                     688284, 2.87
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           6.10%
Status Codes  [code:count]                      0:212682  200:14644  500:12674
```

##### 100ms latency, 1000rps

```
Requests      [total, rate, throughput]         300000, 1000.00, 40.08
Duration      [total, attack, wait]             5m29s, 5m0s, 28.885s
Latencies     [min, mean, 50, 90, 95, 99, max]  7.199ms, 21.159s, 26.998s, 30s, 30s, 30.001s, 30.535s
Bytes In      [total, mean]                     392238, 1.31
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           4.39%
Status Codes  [code:count]                      0:279781  200:13183  500:7036
```

##### 500ms latency, 1000rps

```
Requests      [total, rate, throughput]         300000, 1000.00, 41.96
Duration      [total, attack, wait]             5m30s, 5m0s, 29.878s
Latencies     [min, mean, 50, 90, 95, 99, max]  7.177ms, 20.67s, 26.481s, 30s, 30s, 30.003s, 30.796s
Bytes In      [total, mean]                     403384, 1.34
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           4.61%
Status Codes  [code:count]                      0:278933  200:13842  500:7225
```

##### 1000ms latency, 1000rps

```
Requests      [total, rate, throughput]         300000, 1000.00, 37.08
Duration      [total, attack, wait]             5m30s, 5m0s, 29.994s
Latencies     [min, mean, 50, 90, 95, 99, max]  7.288ms, 20.796s, 26.399s, 30s, 30s, 30.001s, 30.585s
Bytes In      [total, mean]                     330490, 1.10
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           4.08%
Status Codes  [code:count]                      0:281880  200:12235  500:5885
```

##### Summary

The results of these tests are pretty poor: both in terms of low success rate
and high latency, but I would say that they are comparable to the results of
the tests with OriginSimulator serving responses for both origins. The errors
in the logs are the same: `pool_full`. The poor performance is basically
explained by the fact that Belfrage makes twice as many requests to origins
than the test rate.

# Overall summary

I repeated the previous tests we ran and there was no performance degradation
compared to previous tests. I also ran some extra tests which generated higher
load and didn't notice any unexpected errors or metrics data.

One thing I noticed during the tests that generated relatively high load is
that rendering an internal response (i.e. the default error pages that Belfrage
produces) starts to take significant time (up to 900ms), e.g.:

![High internal response generation
latency](./img/2021-10-20-cascade/high_internal_response_generation_latency.png)

I don't think this is something new or something that the cascade functionality
introduced though, we just didn't have these timing metrics before. Just
thought that I'd mention this as it's the only thing that looked interesting
during the tests that I ran.
